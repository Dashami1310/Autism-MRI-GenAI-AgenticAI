{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dashami1310/Autism-MRI-GenAI-AgenticAI/blob/main/GEN_AI_%26_AGENTIC_AI_APPROACH_FOR_IDENTIFICATION_AND_CLASSIFICATION_OF_AUTISM_SPECTRUM_DISORDER_IN_MRI_IMAGES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3lWQf9irIAt"
      },
      "outputs": [],
      "source": [
        "# Cell 1 â€” Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 â€” Install required packages\n",
        "!pip install -q nibabel torch torchvision grad-cam gradio\n",
        "# (If torch not present or you want a specific version, install torch separately)\n"
      ],
      "metadata": {
        "id": "4Mxzgu7IrQti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 â€” Copy and unzip dataset from Google Drive to Colab\n",
        "# CHANGE the source path below if your ZIP is at a different Drive location.\n",
        "!cp \"/content/drive/MyDrive/archive (1).zip\" /content/dataset.zip\n",
        "!unzip -o /content/dataset.zip -d /content/autism_data\n",
        "# list to confirm\n",
        "!ls -R /content/autism_data | sed -n '1,200p'\n",
        "# Just check Drive copy\n",
        "!ls \"/content/drive/MyDrive/autism_data\"\n"
      ],
      "metadata": {
        "id": "LLqchmWOrSI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”½ Copy extracted dataset to Drive permanently (run only ONCE)\n",
        "#!cp -r /content/autism_data \"/content/drive/MyDrive/autism_data\"\n"
      ],
      "metadata": {
        "id": "13zPTvhCH6I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 â€” Set DATA_DIR and count .nii files per class\n",
        "import os\n",
        "DATA_DIR = \"/content/drive/MyDrive/autism_data/Autism\"   # correct for your extracted structure\n",
        "\n",
        "for cls in sorted(os.listdir(DATA_DIR)):\n",
        "    path = os.path.join(DATA_DIR, cls)\n",
        "    if os.path.isdir(path):\n",
        "        nii_count = len([f for f in os.listdir(path) if f.endswith(\".nii\")])\n",
        "        print(f\"{cls}: {nii_count} files\")\n"
      ],
      "metadata": {
        "id": "pF3nnqcCrUTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 â€” Preview a middle slice from one sample .nii file\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "sample_class = sorted(os.listdir(DATA_DIR))[0]\n",
        "sample_folder = os.path.join(DATA_DIR, sample_class)\n",
        "sample_file = [f for f in os.listdir(sample_folder) if f.endswith(\".nii\")][0]\n",
        "nii = nib.load(os.path.join(sample_folder, sample_file))\n",
        "data = nii.get_fdata()\n",
        "mid = data[:, :, data.shape[2] // 2]\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.imshow(mid, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.title(f\"{sample_class} / {sample_file} â€” middle slice\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oJ_W5MlNrXQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 â€” PyTorch Dataset for NIfTI (.nii) files (middle-slice extraction)\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import nibabel as nib\n",
        "\n",
        "class AutismNiiDataset(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        classes = sorted([d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))])\n",
        "        self.classes = classes\n",
        "        for label, cls in enumerate(classes):\n",
        "            cls_path = os.path.join(root, cls)\n",
        "            for f in os.listdir(cls_path):\n",
        "                if f.endswith('.nii'):\n",
        "                    self.samples.append((os.path.join(cls_path, f), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        nii = nib.load(path)\n",
        "        data = nii.get_fdata()\n",
        "        slice_idx = data.shape[2] // 2\n",
        "        img = data[:, :, slice_idx]\n",
        "        img = (img - np.min(img)) / (np.max(img) - np.min(img) + 1e-5)\n",
        "        img = (img * 255).astype(np.uint8)\n",
        "        pil = Image.fromarray(img)\n",
        "        if self.transform:\n",
        "            pil = self.transform(pil)\n",
        "        return pil, label\n"
      ],
      "metadata": {
        "id": "boqb4ZOkraJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 â€” Transforms, dataset creation, and dataloaders\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 8  # change if GPU OOM\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Lambda(lambda im: im.convert('RGB')),\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Lambda(lambda im: im.convert('RGB')),\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "dataset = AutismNiiDataset(DATA_DIR, transform=train_tf)\n",
        "n = len(dataset)\n",
        "train_size = int(0.8 * n)\n",
        "val_size = n - train_size\n",
        "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "# set validation transform\n",
        "val_ds.dataset.transform = val_tf\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"Classes:\", dataset.classes)\n",
        "print(\"Train samples:\", len(train_ds), \"Val samples:\", len(val_ds))\n"
      ],
      "metadata": {
        "id": "43eXZOGOrdKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 â€” Simple Convolutional VAE to generate synthetic MRI slices (optional)\n",
        "# Trains for a few epochs and saves synthetic images into /content/autism_data/synthetic\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Small conv-VAE\n",
        "class ConvVAE(nn.Module):\n",
        "    def __init__(self, z_dim=64):\n",
        "        super().__init__()\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Conv2d(3,32,4,2,1), nn.ReLU(),\n",
        "            nn.Conv2d(32,64,4,2,1), nn.ReLU(),\n",
        "            nn.Conv2d(64,128,4,2,1), nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        # compute flattened size by a dummy forward\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1,3,IMG_SIZE,IMG_SIZE)\n",
        "            flat = self.enc(dummy).shape[1]\n",
        "        self.fc_mu = nn.Linear(flat, z_dim)\n",
        "        self.fc_logvar = nn.Linear(flat, z_dim)\n",
        "        self.dec_input = nn.Linear(z_dim, flat)\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.Unflatten(1, (128, IMG_SIZE//8, IMG_SIZE//8)),\n",
        "            nn.ConvTranspose2d(128,64,4,2,1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64,32,4,2,1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32,3,4,2,1), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.enc(x)\n",
        "        return self.fc_mu(h), self.fc_logvar(h)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = self.dec_input(z)\n",
        "        return self.dec(h)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        out = self.decode(z)\n",
        "        return out, mu, logvar\n",
        "\n",
        "# train VAE briefly on a subset (fast)\n",
        "vae = ConvVAE(z_dim=64).to(device)\n",
        "opt = optim.Adam(vae.parameters(), lr=1e-3)\n",
        "recon_loss = nn.MSELoss(reduction='sum')\n",
        "\n",
        "EPOCHS_VAE = 3  # keep small to save time; increase if you want better quality\n",
        "os.makedirs('/content/autism_data/synthetic', exist_ok=True)\n",
        "\n",
        "# Use small dataloader for VAE training (use train_loader but reduce)\n",
        "vae_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2)\n",
        "\n",
        "for epoch in range(EPOCHS_VAE):\n",
        "    vae.train()\n",
        "    total = 0\n",
        "    for imgs, _ in vae_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        out, mu, logvar = vae(imgs)\n",
        "        loss_recon = recon_loss(out, imgs)\n",
        "        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        loss = loss_recon + kld\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total += loss.item()\n",
        "    print(f\"VAE Epoch {epoch+1}/{EPOCHS_VAE} Loss: {total/len(vae_loader):.2f}\")\n",
        "\n",
        "# Generate and save some synthetic images\n",
        "vae.eval()\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(20, 64).to(device)\n",
        "    samples = vae.decode(z)\n",
        "    for i, s in enumerate(samples):\n",
        "        save_image(s, f\"/content/autism_data/synthetic/synth_{i:02d}.png\")\n",
        "print(\"Saved synthetic images to /content/autism_data/synthetic (you can inspect them)\")\n"
      ],
      "metadata": {
        "id": "dzFlzf5lrgRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 â€” Load pretrained ResNet50, freeze early layers, set final fc\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import torch\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = models.resnet50(weights=\"IMAGENET1K_V2\")\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "for p in model.layer4.parameters():\n",
        "    p.requires_grad = True\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "for p in model.fc.parameters():\n",
        "    p.requires_grad = True\n",
        "model = model.to(DEVICE)\n",
        "print(\"Model on device:\", DEVICE)\n",
        "print(\"Trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
      ],
      "metadata": {
        "id": "bqbT_AuLrjUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“Œ Cell 10 â€” Initialize Grad-CAM with New API\n",
        "\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "# Pick the last convolution layer of ResNet50\n",
        "target_layer = model.layer4[-1]\n",
        "\n",
        "# âœ… No use_cuda parameter in latest versions\n",
        "cam = GradCAM(model=model, target_layers=[target_layer])\n",
        "\n",
        "def generate_gradcam_tensor(input_tensor):\n",
        "    \"\"\"\n",
        "    Generates Grad-CAM heatmap for a given image tensor.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Convert input shape: (1, 3, 224, 224)\n",
        "    grayscale_cam = cam(input_tensor=input_tensor, targets=None)\n",
        "\n",
        "    # GradCAM outputs batch â†’ take the first element\n",
        "    grayscale_cam = grayscale_cam[0]\n",
        "\n",
        "    return grayscale_cam\n"
      ],
      "metadata": {
        "id": "NZTMsuYrrmYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11 â€” Training loop (fine-tune last conv + fc)\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
        "\n",
        "NUM_EPOCHS = 6\n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # training\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    avg_loss = running_loss / (len(train_loader) if len(train_loader)>0 else 1)\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            preds = model(xb).argmax(dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "    val_acc = correct / total if total>0 else 0.0\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} Loss={avg_loss:.4f} ValAcc={val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/best_asd_resnet50.pth\")\n",
        "        print(\"Saved best model: best_asd_resnet50.pth\")\n",
        "\n",
        "print(\"Training finished. Best val acc:\", best_acc)\n"
      ],
      "metadata": {
        "id": "ME82mfEsrpE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11.2 â€” Training/Validation Accuracy Curve with Zoomed Y-axis\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load the saved best model (optional, to display final accuracy)\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_path = \"/content/drive/MyDrive/best_asd_resnet50.pth\"\n",
        "\n",
        "temp_model = models.resnet50(weights=None)\n",
        "temp_model.fc = nn.Linear(temp_model.fc.in_features, 2)\n",
        "temp_model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "temp_model.to(DEVICE)\n",
        "temp_model.eval()\n",
        "\n",
        "# Number of epochs (same as training)\n",
        "NUM_EPOCHS = 6\n",
        "\n",
        "# Record validation accuracy for each epoch\n",
        "val_accuracies = []\n",
        "\n",
        "print(\"Re-evaluating model for generating accuracy curve...\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            preds = temp_model(xb).argmax(dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "    val_acc = correct / total\n",
        "    val_accuracies.append(val_acc)\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "# Plot accuracy curve\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, NUM_EPOCHS+1), val_accuracies, marker='o', color='blue', label='Validation Accuracy')\n",
        "plt.title(\"Validation Accuracy Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.ylim(min(val_accuracies)-0.002, max(val_accuracies)+0.002)  # Zoom in to show small differences\n",
        "plt.xticks(range(1, NUM_EPOCHS+1))\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_3HnXKCktF5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12 â€” Load model, define prediction and Grad-CAM overlay utilities\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Ensure DEVICE, model architecture are available\n",
        "IMG_SIZE = 224\n",
        "preprocess_tf = T.Compose([\n",
        "    T.Lambda(lambda im: im.convert('RGB')),\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "def load_trained_model(path=\"/content/drive/MyDrive/best_asd_resnet50.pth\"):\n",
        "    m = models.resnet50(weights=None)\n",
        "    m.fc = nn.Linear(m.fc.in_features, 2)\n",
        "    m.load_state_dict(torch.load(path, map_location=DEVICE))\n",
        "    m.to(DEVICE)\n",
        "    m.eval()\n",
        "    return m\n",
        "\n",
        "\n",
        "# if you saved a model during training, load into ui_model; otherwise use current model\n",
        "try:\n",
        "    ui_model = load_trained_model(\"/content/drive/MyDrive/best_asd_resnet50.pth\")\n",
        "    print(\"Loaded trained model.\")\n",
        "except Exception as e:\n",
        "    ui_model = model\n",
        "    print(\"Using current model in memory. (No saved file found.)\", e)\n",
        "\n",
        "def nii_to_pil(path):\n",
        "    nii = nib.load(path)\n",
        "    data = nii.get_fdata()\n",
        "    mid = data[:, :, data.shape[2] // 2]\n",
        "    img = ((mid - mid.min()) / (mid.max() - mid.min() + 1e-5) * 255).astype(np.uint8)\n",
        "    return Image.fromarray(img)\n",
        "\n",
        "def predict_and_explain(pil_img, model_local=ui_model):\n",
        "    # preprocess\n",
        "    x = preprocess_tf(pil_img).unsqueeze(0).to(DEVICE)\n",
        "    # prediction\n",
        "    with torch.no_grad():\n",
        "        logits = model_local(x)\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "        pred_idx = int(probs.argmax())\n",
        "        label = dataset.classes[pred_idx]\n",
        "        confidence = float(probs[pred_idx])\n",
        "    # grad-cam (use unnormalized image for show_cam_on_image)\n",
        "    cam_mask = generate_gradcam_tensor(input_tensor=x)\n",
        "    img_for_show = np.array(pil_img.resize((IMG_SIZE, IMG_SIZE))).astype(np.float32) / 255.0\n",
        "    cam_overlay = show_cam_on_image(img_for_show, cam_mask, use_rgb=True)\n",
        "    return label, confidence, cam_overlay\n"
      ],
      "metadata": {
        "id": "if2oad5Trs0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Gradio Interface with Grad-CAM\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load model\n",
        "model = resnet50(pretrained=False)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 2)  # binary classifier\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/best_asd_resnet50.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Transform for input\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Grayscale(num_output_channels=3),  # convert 1 channel to 3 for ResNet\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Grad-CAM helper\n",
        "def gradcam_overlay(model, input_tensor, target_class):\n",
        "    activations = []\n",
        "    gradients = []\n",
        "\n",
        "    def forward_hook(module, input, output):\n",
        "        activations.append(output.detach())\n",
        "\n",
        "    def backward_hook(module, grad_in, grad_out):\n",
        "        gradients.append(grad_out[0].detach())\n",
        "\n",
        "    # Get last conv layer (safe for ResNet50)\n",
        "    last_conv = None\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            last_conv = module\n",
        "    if last_conv is None:\n",
        "        raise ValueError(\"No Conv2d layer found in model\")\n",
        "\n",
        "    h1 = last_conv.register_forward_hook(forward_hook)\n",
        "    h2 = last_conv.register_backward_hook(backward_hook)\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(input_tensor)\n",
        "    pred_class = output.argmax(dim=1)\n",
        "    model.zero_grad()\n",
        "    loss = output[0, target_class]\n",
        "    loss.backward()\n",
        "\n",
        "    activation = activations[0][0]\n",
        "    gradient = gradients[0][0]\n",
        "\n",
        "    weights = gradient.mean(dim=(1, 2), keepdim=True)\n",
        "    cam = (weights * activation).sum(dim=0)\n",
        "    cam = torch.relu(cam)\n",
        "    cam = cam - cam.min()\n",
        "    cam = cam / cam.max()\n",
        "    cam = cam.cpu().numpy()\n",
        "    cam = cv2.resize(cam, (224, 224))\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    img_np = input_tensor[0].permute(1,2,0).cpu().numpy()\n",
        "    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
        "    overlay = 0.5 * heatmap/255 + 0.5 * img_np\n",
        "    overlay = np.uint8(overlay*255)\n",
        "\n",
        "    h1.remove()\n",
        "    h2.remove()\n",
        "\n",
        "    return overlay\n",
        "\n",
        "# Prediction function\n",
        "def predict_nii(file):\n",
        "    nii = nib.load(file.name)\n",
        "    data = nii.get_fdata()\n",
        "    # Take middle slice\n",
        "    mid_slice = data[:, :, data.shape[2]//2]\n",
        "    img = (mid_slice - mid_slice.min()) / (mid_slice.max() - mid_slice.min())\n",
        "    img = np.uint8(img*255)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    img_tensor = test_transform(img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(img_tensor)\n",
        "        pred_class = output.argmax(dim=1)\n",
        "        conf = torch.softmax(output, dim=1)[0, pred_class].item()\n",
        "        label = \"Autistic\" if pred_class.item()==0 else \"Non-Autistic\"\n",
        "\n",
        "    # Grad-CAM\n",
        "    cam_image = gradcam_overlay(model, img_tensor, pred_class.item())\n",
        "\n",
        "    return label + f\" ({conf*100:.1f}%)\", cam_image\n",
        "\n",
        "# Gradio Interface\n",
        "interface = gr.Interface(\n",
        "    fn=predict_nii,\n",
        "    inputs=gr.File(label=\".nii Upload (NIfTI)\"),\n",
        "    outputs=[gr.Textbox(label=\"Prediction\"), gr.Image(label=\"Grad-CAM Overlay\")],\n",
        "    title=\"Autism MRI Prediction with Grad-CAM\"\n",
        ")\n",
        "\n",
        "interface.launch(share=True)\n"
      ],
      "metadata": {
        "id": "KMJhMuzqrvVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”½ Copy the extracted autism_data folder to Google Drive (run this once after extracting the dataset)\n",
        "#!cp -r /content/autism_data \"/content/drive/MyDrive/autism_data\"\n"
      ],
      "metadata": {
        "id": "y0zyz7QPG-Wq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}